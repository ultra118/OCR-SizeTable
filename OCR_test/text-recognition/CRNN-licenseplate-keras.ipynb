{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참조 링크 : https://github.com/qjadud1994/CRNN-Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model\n",
    "\n",
    "## [CTC loss](https://m.blog.naver.com/PostView.nhn?blogId=sogangori&logNo=221183469708&proxyReferer=https%3A%2F%2Fwww.google.com%2F)\n",
    "> CTC는 학습데이터에 클래스 라벨만 순서대로 있고 각 클래스의 위치는 어디있는지 모르는 unsegmented 시퀀스 데이터의 학습을 위해서 사용하는 알고리즘\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/28910538/59762917-4bdc6300-92d3-11e9-9d54-31c6c63ae3a0.png)\n",
    "- RNN은 시퀀스 데이터에대한 학습이 용이함\n",
    "- 그 대신 각 노드에 대한 segmantation이 잘되어있어야함\n",
    "- 근데 이런 segmatation을 해주지 않은 unsegmentated data에 대해서도 학습을 잘 시킬 수 있는 방법은 CTC(Connectionist Temporal Classification)알고리즘을 사용하도록 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_VECTOR = \"adefghjknqrstwABCDEFGHIJKLMNOPZ0123456789\"\n",
    "# one-hot으로 쓰기위한 letters \n",
    "\n",
    "letters = [letter for letter in CHAR_VECTOR]\n",
    "\n",
    "num_classes = len(letters) + 1\n",
    "\n",
    "img_w, img_h = 128, 64\n",
    "\n",
    "# Network parameters\n",
    "# batch_size = 128\n",
    "# val_batch_size = 16\n",
    "batch_size = 1\n",
    "val_batch_size = 1\n",
    "\n",
    "downsample_factor = 4\n",
    "max_text_len = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.layers import Reshape, Lambda, BatchNormalization\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.recurrent import LSTM\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "# # Loss and train functions, network architecture\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    # the 2 is critical here since the first couple outputs of the RNN\n",
    "    # tend to be garbage:\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "\n",
    "def get_Model(training):\n",
    "    input_shape = (img_w, img_h, 1)     # (128, 64, 1)\n",
    "\n",
    "    # Make Networkw\n",
    "    inputs = Input(name='the_input', shape=input_shape, dtype='float32')  # (None, 128, 64, 1)\n",
    "\n",
    "    # Convolution layer (VGG)\n",
    "    inner = Conv2D(64, (3, 3), padding='same', name='conv1', kernel_initializer='he_normal')(inputs)  # (None, 128, 64, 64)\n",
    "    inner = BatchNormalization()(inner)\n",
    "    inner = Activation('relu')(inner)\n",
    "    inner = MaxPooling2D(pool_size=(2, 2), name='max1')(inner)  # (None,64, 32, 64)\n",
    "\n",
    "    inner = Conv2D(128, (3, 3), padding='same', name='conv2', kernel_initializer='he_normal')(inner)  # (None, 64, 32, 128)\n",
    "    inner = BatchNormalization()(inner)\n",
    "    inner = Activation('relu')(inner)\n",
    "    inner = MaxPooling2D(pool_size=(2, 2), name='max2')(inner)  # (None, 32, 16, 128)\n",
    "\n",
    "    inner = Conv2D(256, (3, 3), padding='same', name='conv3', kernel_initializer='he_normal')(inner)  # (None, 32, 16, 256)\n",
    "    inner = BatchNormalization()(inner)\n",
    "    inner = Activation('relu')(inner)\n",
    "    inner = Conv2D(256, (3, 3), padding='same', name='conv4', kernel_initializer='he_normal')(inner)  # (None, 32, 16, 256)\n",
    "    inner = BatchNormalization()(inner)\n",
    "    inner = Activation('relu')(inner)\n",
    "    inner = MaxPooling2D(pool_size=(1, 2), name='max3')(inner)  # (None, 32, 8, 256)\n",
    "\n",
    "    inner = Conv2D(512, (3, 3), padding='same', name='conv5', kernel_initializer='he_normal')(inner)  # (None, 32, 8, 512)\n",
    "    inner = BatchNormalization()(inner)\n",
    "    inner = Activation('relu')(inner)\n",
    "    inner = Conv2D(512, (3, 3), padding='same', name='conv6')(inner)  # (None, 32, 8, 512)\n",
    "    inner = BatchNormalization()(inner)\n",
    "    inner = Activation('relu')(inner)\n",
    "    inner = MaxPooling2D(pool_size=(1, 2), name='max4')(inner)  # (None, 32, 4, 512)\n",
    "\n",
    "    inner = Conv2D(512, (2, 2), padding='same', kernel_initializer='he_normal', name='con7')(inner)  # (None, 32, 4, 512)\n",
    "    inner = BatchNormalization()(inner)\n",
    "    inner = Activation('relu')(inner)\n",
    "\n",
    "    # CNN to RNN\n",
    "    inner = Reshape(target_shape=((32, 2048)), name='reshape')(inner)  # (None, 32, 2048)\n",
    "    inner = Dense(64, activation='relu', kernel_initializer='he_normal', name='dense1')(inner)  # (None, 32, 64)\n",
    "\n",
    "    # RNN layer\n",
    "    lstm_1 = LSTM(256, return_sequences=True, kernel_initializer='he_normal', name='lstm1')(inner)  # (None, 32, 512)\n",
    "    lstm_1b = LSTM(256, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='lstm1_b')(inner)\n",
    "    reversed_lstm_1b = Lambda(lambda inputTensor: K.reverse(inputTensor, axes=1)) (lstm_1b)\n",
    "\n",
    "    lstm1_merged = add([lstm_1, reversed_lstm_1b])  # (None, 32, 512)\n",
    "    lstm1_merged = BatchNormalization()(lstm1_merged)\n",
    "    \n",
    "    lstm_2 = LSTM(256, return_sequences=True, kernel_initializer='he_normal', name='lstm2')(lstm1_merged)\n",
    "    lstm_2b = LSTM(256, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='lstm2_b')(lstm1_merged)\n",
    "    reversed_lstm_2b= Lambda(lambda inputTensor: K.reverse(inputTensor, axes=1)) (lstm_2b)\n",
    "\n",
    "    lstm2_merged = concatenate([lstm_2, reversed_lstm_2b])  # (None, 32, 1024)\n",
    "    lstm_merged = BatchNormalization()(lstm2_merged)\n",
    "\n",
    "    # transforms RNN output to character activations:\n",
    "    inner = Dense(num_classes, kernel_initializer='he_normal',name='dense2')(lstm2_merged) #(None, 32, 63)\n",
    "    y_pred = Activation('softmax', name='softmax')(inner)\n",
    "\n",
    "    labels = Input(name='the_labels', shape=[max_text_len], dtype='float32') # (None ,8)\n",
    "    input_length = Input(name='input_length', shape=[1], dtype='int64')     # (None, 1)\n",
    "    label_length = Input(name='label_length', shape=[1], dtype='int64')     # (None, 1)\n",
    "\n",
    "    # Keras doesn't currently support loss funcs with extra parameters\n",
    "    # so CTC loss is implemented in a lambda layer\n",
    "    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length]) #(None, 1)\n",
    "    print(loss_out)\n",
    "    if training:\n",
    "        return Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)\n",
    "    else:\n",
    "        return Model(inputs=[inputs], outputs=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ctc_1/ExpandDims:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "crnn_model = get_Model(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ctc_6/ExpandDims:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "crnn_predict_model = get_Model(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          (None, 128, 64, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 128, 64, 64)  640         the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 64, 64)  256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 64, 64)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max1 (MaxPooling2D)             (None, 64, 32, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 64, 32, 128)  73856       max1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 32, 128)  512         conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 32, 128)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max2 (MaxPooling2D)             (None, 32, 16, 128)  0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                  (None, 32, 16, 256)  295168      max2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 16, 256)  1024        conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 16, 256)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4 (Conv2D)                  (None, 32, 16, 256)  590080      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 16, 256)  1024        conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 16, 256)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max3 (MaxPooling2D)             (None, 32, 8, 256)   0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv5 (Conv2D)                  (None, 32, 8, 512)   1180160     max3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 8, 512)   2048        conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 8, 512)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv6 (Conv2D)                  (None, 32, 8, 512)   2359808     activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 8, 512)   2048        conv6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 8, 512)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max4 (MaxPooling2D)             (None, 32, 4, 512)   0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "con7 (Conv2D)                   (None, 32, 4, 512)   1049088     max4[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 4, 512)   2048        con7[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 4, 512)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 32, 2048)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 32, 64)       131136      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm1_b (LSTM)                  (None, 32, 256)      328704      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm1 (LSTM)                    (None, 32, 256)      328704      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 32, 256)      0           lstm1_b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 256)      0           lstm1[0][0]                      \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 256)      1024        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm2_b (LSTM)                  (None, 32, 256)      525312      batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lstm2 (LSTM)                    (None, 32, 256)      525312      batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 32, 256)      0           lstm2_b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 512)      0           lstm2[0][0]                      \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 32, 42)       21546       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 32, 42)       0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           softmax[0][0]                    \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 7,419,498\n",
      "Trainable params: 7,414,506\n",
      "Non-trainable params: 4,992\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "crnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          (None, 128, 64, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 128, 64, 64)  640         the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 128, 64, 64)  256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 128, 64, 64)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max1 (MaxPooling2D)             (None, 64, 32, 64)   0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 64, 32, 128)  73856       max1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 64, 32, 128)  512         conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 64, 32, 128)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max2 (MaxPooling2D)             (None, 32, 16, 128)  0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                  (None, 32, 16, 256)  295168      max2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 32, 16, 256)  1024        conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 32, 16, 256)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4 (Conv2D)                  (None, 32, 16, 256)  590080      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 32, 16, 256)  1024        conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 32, 16, 256)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max3 (MaxPooling2D)             (None, 32, 8, 256)   0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5 (Conv2D)                  (None, 32, 8, 512)   1180160     max3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 32, 8, 512)   2048        conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 32, 8, 512)   0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv6 (Conv2D)                  (None, 32, 8, 512)   2359808     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 32, 8, 512)   2048        conv6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 32, 8, 512)   0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max4 (MaxPooling2D)             (None, 32, 4, 512)   0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "con7 (Conv2D)                   (None, 32, 4, 512)   1049088     max4[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 32, 4, 512)   2048        con7[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 32, 4, 512)   0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 32, 2048)     0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 32, 64)       131136      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm1_b (LSTM)                  (None, 32, 256)      328704      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm1 (LSTM)                    (None, 32, 256)      328704      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 32, 256)      0           lstm1_b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 32, 256)      0           lstm1[0][0]                      \n",
      "                                                                 lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 32, 256)      1024        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm2_b (LSTM)                  (None, 32, 256)      525312      batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm2 (LSTM)                    (None, 32, 256)      525312      batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 32, 256)      0           lstm2_b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 32, 512)      0           lstm2[0][0]                      \n",
      "                                                                 lambda_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 32, 42)       21546       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 32, 42)       0           dense2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 7,419,498\n",
      "Trainable params: 7,414,506\n",
      "Non-trainable params: 4,992\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "crnn_predict_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "> 대용량 처리는 메모리에 모두 올려놓고 사용할 수 없으니 한 줄 씩 읽어 반복 처리\n",
    "> 한번에 끝나지 않고 여러번에 걸쳐 입출력을 받을 수 있음\n",
    "### yield\n",
    "- `yield`는 함수 실행 중간에 빠져나올 수 있는 `generator`를 만들 때 사용\n",
    "    - iter가능\n",
    "    - `next`통해 하나씩 값을 뽑을 수도\n",
    "    - 값을 받을수도 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os, random\n",
    "import numpy as np\n",
    "\n",
    "# # Input data generator\n",
    "def labels_to_text(labels):     # letters의 index -> text (string)\n",
    "    return ''.join(list(map(lambda x: letters[int(x)], labels)))\n",
    "\n",
    "def text_to_labels(text):      # text를 letters 배열에서의 인덱스 값으로 변환\n",
    "    return list(map(lambda x: letters.index(x), text))\n",
    "\n",
    "\n",
    "class TextImageGenerator:\n",
    "    def __init__(self, img_dirpath, img_w, img_h,\n",
    "                 batch_size, downsample_factor, max_text_len=9):\n",
    "        self.img_h = img_h\n",
    "        self.img_w = img_w\n",
    "        self.batch_size = batch_size\n",
    "        self.max_text_len = max_text_len\n",
    "        self.downsample_factor = downsample_factor\n",
    "        self.img_dirpath = img_dirpath                  # image dir path\n",
    "        self.img_dir = os.listdir(self.img_dirpath)     # images list\n",
    "        self.n = len(self.img_dir)                      # number of images\n",
    "        self.indexes = list(range(self.n))\n",
    "        self.cur_index = 0\n",
    "        self.imgs = np.zeros((self.n, self.img_h, self.img_w))\n",
    "        self.texts = []\n",
    "\n",
    "    ## samples의 이미지 목록들을 opencv로 읽어 저장하기, texts에는 label 저장\n",
    "    def build_data(self):\n",
    "        print(self.n, \" Image Loading start...\")\n",
    "        for i, img_file in enumerate(self.img_dir):\n",
    "            img = cv2.imread(self.img_dirpath + img_file, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, (self.img_w, self.img_h))\n",
    "            img = img.astype(np.float32)\n",
    "            img = (img / 255.0) * 2.0 - 1.0\n",
    "\n",
    "            self.imgs[i, :, :] = img\n",
    "            self.texts.append(img_file[0:-4])\n",
    "        print(len(self.texts) == self.n)\n",
    "        print(self.n, \" Image Loading finish...\")\n",
    "\n",
    "    def next_sample(self):      ## index max -> 0 으로 만들기\n",
    "        self.cur_index += 1\n",
    "        if self.cur_index >= self.n:\n",
    "            self.cur_index = 0\n",
    "            random.shuffle(self.indexes)\n",
    "        return self.imgs[self.indexes[self.cur_index]], self.texts[self.indexes[self.cur_index]]\n",
    "\n",
    "    def next_batch(self):       ## batch size만큼 가져오기\n",
    "        while True:\n",
    "            X_data = np.ones([self.batch_size, self.img_w, self.img_h, 1])     # (bs, 128, 64, 1)\n",
    "            Y_data = np.ones([self.batch_size, self.max_text_len])             # (bs, 9)\n",
    "            input_length = np.ones((self.batch_size, 1)) * (self.img_w // self.downsample_factor - 2)  # (bs, 1)\n",
    "            label_length = np.zeros((self.batch_size, 1))           # (bs, 1)\n",
    "\n",
    "            for i in range(self.batch_size):\n",
    "                img, text = self.next_sample()\n",
    "                img = img.T\n",
    "                img = np.expand_dims(img, -1)\n",
    "                X_data[i] = img\n",
    "                Y_data[i] = text_to_labels(text)\n",
    "                label_length[i] = len(text)\n",
    "                #print(f'Y_data[{i}] : {Y_data[i]}\\ninput_length : {input_length}\\n label_length[{i}]: {label_length[i]}\\n')\n",
    "            \n",
    "            # dict 형태로 복사\n",
    "            inputs = {\n",
    "                'the_input': X_data,  # (bs, 128, 64, 1) --> image\n",
    "                'the_labels': Y_data,  # (bs, 8) --> text를 label로 바꾼 값 (48 17 48 ...)같은\n",
    "                'input_length': input_length,  # (bs, 1) -> 모든 원소 value = 30 --> \n",
    "                'label_length': label_length  # (bs, 1) -> 모든 원소 value = 8 --> label의 길이\n",
    "            }\n",
    "            outputs = {'ctc': np.zeros([self.batch_size])}   # (bs, 1) -> 모든 원소 0\n",
    "            yield (inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "8\n",
      "27\n",
      "64\n",
      "125\n",
      "216\n",
      "343\n",
      "512\n",
      "729\n"
     ]
    }
   ],
   "source": [
    "def gen():\n",
    "    for i in range(10):\n",
    "        yield i ** 3\n",
    "\n",
    "for x in gen():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5  Image Loading start...\n",
      "True\n",
      "5  Image Loading finish...\n"
     ]
    }
   ],
   "source": [
    "train_file_path = './DB/train/'\n",
    "tiger_train = TextImageGenerator(train_file_path, img_w, img_h, batch_size, downsample_factor)\n",
    "tiger_train.build_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 64, 128)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiger_train.imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A18sk6897', 'B16dj0824', 'F06aj4915', 'G11qk8249']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiger_train.texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b1555c0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(tiger_train.imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img, test_text = tiger_train.next_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 31, 37, 0, 6, 35, 40, 32, 36]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_labels(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [EarlyStopping](https://tykimos.github.io/2017/07/09/Early_Stopping/)\n",
    "> 특정 조건을 만족하면 학습을 종료시키는 콜백 함수\n",
    "\n",
    "```python\n",
    "keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')\n",
    "```\n",
    "\n",
    "## [fit_generator](https://tykimos.github.io/2017/03/08/CNN_Getting_Started/)\n",
    "> 제너레이터로 생성된 배치를 학습할 떄 사용\n",
    "\n",
    "- 제너레이터는 대용량의 학습데이터를 메모리에 다 로드해놓을 수 없으니 `yield`를 통해 한번씩 불러와서 학습시키기 위한 방법\n",
    "\n",
    "### 인자\n",
    "- 첫번째 인자 : 훈련데이터셋을 제공할 제네레이터를 지정\n",
    "- steps_per_epoch : 한 epoch에 사용한 스텝 수를 지정합니다. 총 샘플 수 / batch\n",
    "- epochs : 전체 훈련 데이터셋에 대해 학습 반복 횟수를 지정\n",
    "- validation_data : 검증데이터셋을 제공할 제네레이터를 지정\n",
    "- validation_steps : 한 epoch 종료 시 마다 검증할 때 사용되는 검증 스텝 수를 지정 총 검증 샘플 수 / batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ctc_1/ExpandDims:0\", shape=(?, 1), dtype=float32)\n",
      "...New weight data...\n",
      "5  Image Loading start...\n",
      "True\n",
      "5  Image Loading finish...\n",
      "5  Image Loading start...\n",
      "True\n",
      "5  Image Loading finish...\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 9s 2s/step - loss: 84.3499 - val_loss: 85.2833\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 4s 764ms/step - loss: 55.2840 - val_loss: 37.0403\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 4s 787ms/step - loss: 40.2301 - val_loss: 44.7843\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 4s 783ms/step - loss: 40.7926 - val_loss: 36.4645\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 4s 783ms/step - loss: 39.7627 - val_loss: 34.5797\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 4s 779ms/step - loss: 38.3609 - val_loss: 33.6194\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 4s 768ms/step - loss: 36.0357 - val_loss: 36.7630\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 4s 766ms/step - loss: 31.0019 - val_loss: 35.8520\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 4s 766ms/step - loss: 28.5847 - val_loss: 33.2153\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 4s 769ms/step - loss: 28.0172 - val_loss: 33.4419\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 4s 774ms/step - loss: 27.0764 - val_loss: 38.0200\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 4s 779ms/step - loss: 25.7996 - val_loss: 30.8126\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 4s 776ms/step - loss: 24.2890 - val_loss: 39.1735\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 4s 776ms/step - loss: 24.1337 - val_loss: 35.7227\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 4s 773ms/step - loss: 24.1976 - val_loss: 35.8847\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 4s 779ms/step - loss: 22.8656 - val_loss: 38.2753\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 4s 772ms/step - loss: 23.7837 - val_loss: 35.0821\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 4s 774ms/step - loss: 29.2676 - val_loss: 36.5884\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 4s 775ms/step - loss: 27.4590 - val_loss: 36.6199\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 4s 771ms/step - loss: 25.2720 - val_loss: 35.5387\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 4s 764ms/step - loss: 22.2240 - val_loss: 35.2734\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 4s 769ms/step - loss: 21.6487 - val_loss: 36.2742\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 4s 767ms/step - loss: 22.2431 - val_loss: 34.6271\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 4s 779ms/step - loss: 21.7528 - val_loss: 41.8683\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 4s 821ms/step - loss: 21.2750 - val_loss: 38.7474\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 4s 785ms/step - loss: 19.9317 - val_loss: 30.1193\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 4s 760ms/step - loss: 21.0109 - val_loss: 42.8906\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 4s 769ms/step - loss: 19.9721 - val_loss: 38.3948\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 4s 767ms/step - loss: 19.5443 - val_loss: 41.4689\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 4s 766ms/step - loss: 30.1646 - val_loss: 37.3203\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 4s 767ms/step - loss: 21.6093 - val_loss: 38.7469\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 4s 763ms/step - loss: 20.0009 - val_loss: 39.7111\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 4s 762ms/step - loss: 20.9258 - val_loss: 37.7841\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 4s 774ms/step - loss: 19.4448 - val_loss: 37.1165\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 4s 775ms/step - loss: 19.6156 - val_loss: 40.8745\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 4s 762ms/step - loss: 20.8256 - val_loss: 38.6195\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 4s 762ms/step - loss: 18.4194 - val_loss: 42.6509\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 4s 761ms/step - loss: 19.8384 - val_loss: 38.7535\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 4s 764ms/step - loss: 19.0842 - val_loss: 41.3060\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 4s 761ms/step - loss: 21.7373 - val_loss: 33.4190\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 4s 798ms/step - loss: 19.5534 - val_loss: 43.8425\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 4s 797ms/step - loss: 19.1055 - val_loss: 40.7222\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 4s 761ms/step - loss: 23.4972 - val_loss: 36.0433\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 4s 763ms/step - loss: 19.9454 - val_loss: 40.1190\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 4s 765ms/step - loss: 17.5888 - val_loss: 49.0346\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 4s 763ms/step - loss: 15.9863 - val_loss: 41.0863\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 4s 762ms/step - loss: 20.0834 - val_loss: 35.0829\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 4s 773ms/step - loss: 18.4652 - val_loss: 42.3119\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 4s 762ms/step - loss: 21.1681 - val_loss: 39.0511\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 4s 759ms/step - loss: 17.7732 - val_loss: 42.3207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x30f75518>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "# # Model description and training\n",
    "\n",
    "model = get_Model(training=True)\n",
    "\n",
    "try:\n",
    "    model.load_weights('LSTM+BN4--26--0.011.hdf5')\n",
    "    print(\"...Previous weight data...\")\n",
    "except:\n",
    "    print(\"...New weight data...\")\n",
    "    pass\n",
    "\n",
    "train_file_path = './DB/train/'\n",
    "tiger_train = TextImageGenerator(train_file_path, img_w, img_h, batch_size, downsample_factor)\n",
    "tiger_train.build_data()\n",
    "\n",
    "valid_file_path = './DB/test/'\n",
    "tiger_val = TextImageGenerator(valid_file_path, img_w, img_h, val_batch_size, downsample_factor)\n",
    "tiger_val.build_data()\n",
    "\n",
    "ada = Adadelta()\n",
    "# loss의 변화량이 0.001보다 적으면 학습의 개선이 없다 판단하고 4 epoch만큼 지속되면 종료\n",
    "early_stop = EarlyStopping(monitor='loss', min_delta=0.001, patience=4, mode='min', verbose=1)\n",
    "#checkpoint = ModelCheckpoint(filepath='LSTM+BN5--{epoch:02d}--{val_loss:.3f}.hdf5', monitor='loss', verbose=1, mode='min', period=1)\n",
    "# the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n",
    "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=ada)\n",
    "\n",
    "# generator로 생성된 배치로 학습할떄 fit_generator 사용\n",
    "# \n",
    "model.fit_generator(generator=tiger_train.next_batch(),\n",
    "                    steps_per_epoch=int(tiger_train.n / batch_size),\n",
    "                    epochs=50,\n",
    "                    #callbacks=[checkpoint],\n",
    "                    validation_data=tiger_val.next_batch(),\n",
    "                    validation_steps=int(tiger_val.n / val_batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weight save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Hangul.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import itertools, os, time\n",
    "import numpy as np\n",
    "import argparse\n",
    "from keras import backend as K\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "Region = {\"A\": \"서울 \", \"B\": \"경기 \", \"C\": \"인천 \", \"D\": \"강원 \", \"E\": \"충남 \", \"F\": \"대전 \",\n",
    "          \"G\": \"충북 \", \"H\": \"부산 \", \"I\": \"울산 \", \"J\": \"대구 \", \"K\": \"경북 \", \"L\": \"경남 \",\n",
    "          \"M\": \"전남 \", \"N\": \"광주 \", \"O\": \"전북 \", \"P\": \"제주 \"}\n",
    "Hangul = {\"dk\": \"아\", \"dj\": \"어\", \"dh\": \"오\", \"dn\": \"우\", \"qk\": \"바\", \"qj\": \"버\", \"qh\": \"보\", \"qn\": \"부\",\n",
    "          \"ek\": \"다\", \"ej\": \"더\", \"eh\": \"도\", \"en\": \"두\", \"rk\": \"가\", \"rj\": \"거\", \"rh\": \"고\", \"rn\": \"구\",\n",
    "          \"wk\": \"자\", \"wj\": \"저\", \"wh\": \"조\", \"wn\": \"주\", \"ak\": \"마\", \"aj\": \"머\", \"ah\": \"모\", \"an\": \"무\",\n",
    "          \"sk\": \"나\", \"sj\": \"너\", \"sh\": \"노\", \"sn\": \"누\", \"fk\": \"라\", \"fj\": \"러\", \"fh\": \"로\", \"fn\": \"루\",\n",
    "          \"tk\": \"사\", \"tj\": \"서\", \"th\": \"소\", \"tn\": \"수\", \"gj\": \"허\"}\n",
    "\n",
    "#\n",
    "def decode_label(out):\n",
    "    # out : (1, 32, 42)\n",
    "    print(f'decode_label.. out.shape : {out.shape}')\n",
    "    out_best = list(np.argmax(out[0, 2:], axis=1))  # get max index -> len = 32\n",
    "    print(f'decode_label.. out_best : {out_best}')\n",
    "    print(f'decode_label.. out_best.shape : {len(out_best)}')\n",
    "    out_best = [k for k, g in itertools.groupby(out_best)]  # remove overlap value\n",
    "    print(f'decode_label..groupby.. out_best : {out_best}')\n",
    "    outstr = ''\n",
    "    for i in out_best:\n",
    "        if i < len(letters):\n",
    "            outstr += letters[i]\n",
    "    print(f'decode_label..groupby.. outstr : {outstr}')\n",
    "    return outstr\n",
    "\n",
    "\n",
    "# A99th3954같은 형태의 label이 예측되게 되는데 각 label의 위치값으로 번호판 예측\n",
    "def label_to_hangul(label):  # eng -> hangul\n",
    "    region = label[0]\n",
    "    two_num = label[1:3]\n",
    "    hangul = label[3:5]\n",
    "    four_num = label[5:]\n",
    "\n",
    "    try:\n",
    "        region = Region[region] if region != 'Z' else ''\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        hangul = Hangul[hangul]\n",
    "    except:\n",
    "        pass\n",
    "    return region + two_num + hangul + four_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_pred.shape : (1, 128, 64, 1)\n",
      "decode_label.. out : [[[1.77545585e-02 1.27857365e-02 9.20890644e-03 ... 5.54262148e-03\n",
      "   1.66375041e-02 6.79988414e-03]\n",
      "  [4.97288909e-03 5.00057638e-03 3.98675771e-03 ... 3.63771780e-03\n",
      "   1.46003896e-02 3.87180783e-03]\n",
      "  [3.72951594e-03 2.44037411e-03 1.84131647e-03 ... 3.64507106e-03\n",
      "   2.46913470e-02 7.23525370e-03]\n",
      "  ...\n",
      "  [4.31719382e-04 3.62156046e-04 3.15336918e-04 ... 2.65392270e-02\n",
      "   6.38357937e-01 4.89093922e-03]\n",
      "  [5.96296100e-04 5.41131012e-04 5.55351842e-04 ... 4.65710415e-03\n",
      "   2.37670720e-01 3.31204222e-03]\n",
      "  [1.06788299e-03 7.81709852e-04 1.28480012e-03 ... 2.46030930e-03\n",
      "   1.69234216e-01 3.64623778e-03]]]\n",
      "decode_label.. out.shape : (1, 32, 42)\n",
      "decode_label.. out_best : [19, 31, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 39, 40, 36, 36]\n",
      "decode_label.. out_best.shape : 30\n",
      "decode_label..groupby.. out_best : [19, 31, 41, 39, 40, 36]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img = cv2.imread(test_dir + test_img, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "img_pred = img.astype(np.float32)\n",
    "img_pred = cv2.resize(img_pred, (128, 64))\n",
    "img_pred = (img_pred / 255.0) * 2.0 - 1.0\n",
    "img_pred = img_pred.T\n",
    "img_pred = np.expand_dims(img_pred, axis=-1)\n",
    "img_pred = np.expand_dims(img_pred, axis=0)\n",
    "# 1, 128, 64, 1\n",
    "print(f'img_pred.shape : {img_pred.shape}')\n",
    "\n",
    "net_out_value = model.predict(img_pred)\n",
    "#pred_texts = decode_label(net_out_value)\n",
    "out = net_out_value\n",
    "print(f'decode_label.. out : {out}')\n",
    "print(f'decode_label.. out.shape : {out.shape}')\n",
    "out_best = list(np.argmax(out[0, 2:], axis=1))  # get max index -> len = 32\n",
    "print(f'decode_label.. out_best : {out_best}')\n",
    "print(f'decode_label.. out_best.shape : {len(out_best)}')\n",
    "out_best = [k for k, g in itertools.groupby(out_best)]  # remove overlap value\n",
    "print(f'decode_label..groupby.. out_best : {out_best}')\n",
    "outstr = ''\n",
    "for i in out_best:\n",
    "    if i < len(letters):\n",
    "        outstr += letters[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 31, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41,\n",
       "       41, 41, 41, 41, 41, 41, 41, 41, 41, 39, 40, 36, 36], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(out[0, 2:], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21, 19, 19, 31, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41,\n",
       "       41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 39, 40, 36, 36],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(out[0, :], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ctc_3/ExpandDims:0\", shape=(?, 1), dtype=float32)\n",
      "...Previous weight data...\n",
      "img_pred.shape : (1, 128, 64, 1)\n",
      "decode_label.. out.shape : (1, 32, 42)\n",
      "decode_label.. out_best : [20, 32, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 39, 39, 33, 35, 35]\n",
      "decode_label.. out_best.shape : 30\n",
      "decode_label..groupby.. out_best : [20, 32, 41, 39, 33, 35]\n",
      "decode_label..groupby.. outstr : G1824\n",
      "net_out_value.shape : (1, 32, 42) pred_texts : G1824\n",
      "Predicted: 충북 1824  /  True: 부산 99우8954\n",
      "img_pred.shape : (1, 128, 64, 1)\n",
      "decode_label.. out.shape : (1, 32, 42)\n",
      "decode_label.. out_best : [20, 32, 32, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 39, 39, 33, 35, 35]\n",
      "decode_label.. out_best.shape : 30\n",
      "decode_label..groupby.. out_best : [20, 32, 41, 39, 33, 35]\n",
      "decode_label..groupby.. outstr : G1824\n",
      "net_out_value.shape : (1, 32, 42) pred_texts : G1824\n",
      "Predicted: 충북 1824  /  True: 경남 37마0112\n",
      "img_pred.shape : (1, 128, 64, 1)\n",
      "decode_label.. out.shape : (1, 32, 42)\n",
      "decode_label.. out_best : [20, 32, 32, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 39, 40, 40, 35]\n",
      "decode_label.. out_best.shape : 30\n",
      "decode_label..groupby.. out_best : [20, 32, 41, 39, 40, 35]\n",
      "decode_label..groupby.. outstr : G1894\n",
      "net_out_value.shape : (1, 32, 42) pred_texts : G1894\n",
      "Predicted: 충북 1894  /  True: 광주 86오0507\n",
      "img_pred.shape : (1, 128, 64, 1)\n",
      "decode_label.. out.shape : (1, 32, 42)\n",
      "decode_label.. out_best : [20, 32, 32, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 39, 39, 33, 35, 35]\n",
      "decode_label.. out_best.shape : 30\n",
      "decode_label..groupby.. out_best : [20, 32, 41, 39, 33, 35]\n",
      "decode_label..groupby.. outstr : G1824\n",
      "net_out_value.shape : (1, 32, 42) pred_texts : G1824\n",
      "Predicted: 충북 1824  /  True: 전북 61누2386\n",
      "img_pred.shape : (1, 128, 64, 1)\n",
      "decode_label.. out.shape : (1, 32, 42)\n",
      "decode_label.. out_best : [20, 32, 32, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 39, 39, 33, 35, 35]\n",
      "decode_label.. out_best.shape : 30\n",
      "decode_label..groupby.. out_best : [20, 32, 41, 39, 33, 35]\n",
      "decode_label..groupby.. outstr : G1824\n",
      "net_out_value.shape : (1, 32, 42) pred_texts : G1824\n",
      "Predicted: 충북 1824  /  True: 제주 57아3309\n",
      "Time :  0.3148180484771729\n",
      "ACC :  0.0\n",
      "letter ACC :  0.0\n"
     ]
    }
   ],
   "source": [
    "model = get_Model(training=False)\n",
    "\n",
    "try:\n",
    "    model.load_weights('model.h5')\n",
    "    print(\"...Previous weight data...\")\n",
    "except:\n",
    "    raise Exception(\"No weight file!\")\n",
    "\n",
    "test_dir = './DB/test/'\n",
    "test_imgs = os.listdir(test_dir)\n",
    "total = 0\n",
    "acc = 0\n",
    "letter_total = 0\n",
    "letter_acc = 0\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "for test_img in test_imgs:\n",
    "    img = cv2.imread(test_dir + test_img, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    img_pred = img.astype(np.float32)\n",
    "    img_pred = cv2.resize(img_pred, (128, 64))\n",
    "    img_pred = (img_pred / 255.0) * 2.0 - 1.0\n",
    "    img_pred = img_pred.T\n",
    "    img_pred = np.expand_dims(img_pred, axis=-1)\n",
    "    img_pred = np.expand_dims(img_pred, axis=0)\n",
    "    # 1, 128, 64, 1\n",
    "    print(f'img_pred.shape : {img_pred.shape}')\n",
    "\n",
    "    net_out_value = model.predict(img_pred)\n",
    "\n",
    "    pred_texts = decode_label(net_out_value)\n",
    "    print(f'net_out_value.shape : {net_out_value.shape} pred_texts : {pred_texts}')\n",
    "    \n",
    "          \n",
    "    for i in range(min(len(pred_texts), len(test_img[0:-4]))):\n",
    "        if pred_texts[i] == test_img[i]:\n",
    "            letter_acc += 1\n",
    "    letter_total += max(len(pred_texts), len(test_img[0:-4]))\n",
    "\n",
    "    if pred_texts == test_img[0:-4]:\n",
    "        acc += 1\n",
    "    total += 1\n",
    "    print('Predicted: %s  /  True: %s' % (label_to_hangul(pred_texts), label_to_hangul(test_img[0:-4])))\n",
    "    \n",
    "    # cv2.rectangle(img, (0,0), (150, 30), (0,0,0), -1)\n",
    "    # cv2.putText(img, pred_texts, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255),2)\n",
    "\n",
    "    #cv2.imshow(\"q\", img)\n",
    "    #if cv2.waitKey(0) == 27:\n",
    "    #   break\n",
    "    #cv2.destroyAllWindows()\n",
    "\n",
    "end = time.time()\n",
    "total_time = (end - start)\n",
    "print(\"Time : \",total_time / total)\n",
    "print(\"ACC : \", acc / total)\n",
    "print(\"letter ACC : \", letter_acc / letter_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = test_imgs[0]\n",
    "img = cv2.imread(test_dir + test_img, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "img_pred = img.astype(np.float32)\n",
    "img_pred = cv2.resize(img_pred, (128, 64))\n",
    "img_pred = (img_pred / 255.0) * 2.0 - 1.0\n",
    "print(img_pred.shape)\n",
    "img_pred = img_pred.T\n",
    "img_pred = np.expand_dims(img_pred, axis=-1)\n",
    "img_pred = np.expand_dims(img_pred, axis=0)\n",
    "print(f'img_pred.shape : {img_pred.shape}')\n",
    "\n",
    "net_out_value = model.predict(img_pred)\n",
    "\n",
    "pred_texts = decode_label(net_out_value)\n",
    "print(f'net_out_value.shape : {net_out_value.shape}')\n",
    "print(f'pred_texts : {pred_texts}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tmh_env] *",
   "language": "python",
   "name": "conda-env-tmh_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
