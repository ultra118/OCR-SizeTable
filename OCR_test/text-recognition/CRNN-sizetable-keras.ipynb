{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train data 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './data/train'\n",
    "test_path = './data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_len = int(len(os.listdir(train_path)) * 0.8) \n",
    "test_set_len = len(os.listdir(train_path)) - train_set_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = []\n",
    "for file in os.listdir(train_path):\n",
    "    file_path = f'{train_path}/{file}'\n",
    "    file_list.append(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/font1_c1n1b1_101d.jpg'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list[0][12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in random.sample(file_list, test_set_len):\n",
    "    move_dir = f'{test_path}{file[0][12:]}'\n",
    "    shutil.move(file, move_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6480"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(test_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_VECTOR = \"0123456789d\"\n",
    "# one-hot으로 쓰기위한 letters \n",
    "\n",
    "letters = [letter for letter in CHAR_VECTOR]\n",
    "\n",
    "num_classes = len(letters) + 1\n",
    "\n",
    "img_w, img_h = 92, 32\n",
    "\n",
    "# Network parameters\n",
    "# batch_size = 128\n",
    "# val_batch_size = 16\n",
    "batch_size = 128\n",
    "val_batch_size = 1\n",
    "\n",
    "downsample_factor = 4\n",
    "max_text_len = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.layers import Reshape, Lambda, BatchNormalization\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.recurrent import LSTM\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "# # Loss and train functions, network architecture\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    # the 2 is critical here since the first couple outputs of the RNN\n",
    "    # tend to be garbage:\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "\n",
    "def get_Model(training):\n",
    "    input_shape = (img_w, img_h, 1)     # (90, 30, 1)\n",
    "\n",
    "    # Make Networkw\n",
    "    inputs = Input(name='the_input', shape=input_shape, dtype='float32')  # (None, 90, 30, 1)\n",
    "\n",
    "    # Convolution layer (VGG)\n",
    "    inner = Conv2D(64, (3, 3), padding='same', name='conv1', kernel_initializer='he_normal')(inputs)  # (None, 48, 56, 32)\n",
    "    inner = BatchNormalization()(inner)\n",
    "    inner = Activation('relu')(inner)\n",
    "    inner = MaxPooling2D(pool_size=(2, 2), name='max1')(inner)  # (None, 32, 16, 128)\n",
    "\n",
    "    inner = Conv2D(128, (3, 3), padding='same', name='conv2', kernel_initializer='he_normal')(inner)  # (None, 24, 56, 64)\n",
    "    inner = BatchNormalization()(inner)\n",
    "    inner = Activation('relu')(inner)\n",
    "    inner = MaxPooling2D(pool_size=(2, 2), name='max2')(inner)  # (None, 32, 16, 128)\n",
    "\n",
    "    inner = Conv2D(256, (2, 2), padding='same', name='conv6')(inner)  # (None, 32, 8, 512)\n",
    "    inner = BatchNormalization()(inner)\n",
    "    inner = Activation('relu')(inner)\n",
    "    inner = MaxPooling2D(pool_size=(1, 2), name='max4')(inner)  # (None, 32, 4, 512)\n",
    "\n",
    "    inner = Conv2D(256, (2, 2), padding='same', kernel_initializer='he_normal', name='con7')(inner)  # (None, 32, 4, 512)\n",
    "    inner = BatchNormalization()(inner)\n",
    "    inner = Activation('relu')(inner)\n",
    "    print(inner)\n",
    "    # CNN to RNN\n",
    "    inner = Reshape(target_shape=((23, 4*256)), name='reshape')(inner)  # (None, 32, 2048)\n",
    "    inner = Dense(128, activation='relu', kernel_initializer='he_normal', name='dense1')(inner)  # (None, 32, 64)\n",
    "\n",
    "    # RNN layer\n",
    "    lstm_1 = LSTM(256, return_sequences=True, kernel_initializer='he_normal', name='lstm1')(inner)  # (None, 32, 512)\n",
    "    lstm_1b = LSTM(256, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='lstm1_b')(inner)\n",
    "    reversed_lstm_1b = Lambda(lambda inputTensor: K.reverse(inputTensor, axes=1)) (lstm_1b)\n",
    "\n",
    "    lstm1_merged = add([lstm_1, reversed_lstm_1b])  # (None, 32, 512)\n",
    "    lstm1_merged = BatchNormalization()(lstm1_merged)\n",
    "    \n",
    "    lstm_2 = LSTM(256, return_sequences=True, kernel_initializer='he_normal', name='lstm2')(lstm1_merged)\n",
    "    lstm_2b = LSTM(256, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='lstm2_b')(lstm1_merged)\n",
    "    reversed_lstm_2b= Lambda(lambda inputTensor: K.reverse(inputTensor, axes=1)) (lstm_2b)\n",
    "\n",
    "    lstm2_merged = concatenate([lstm_2, reversed_lstm_2b])  # (None, 32, 1024)\n",
    "    lstm_merged = BatchNormalization()(lstm2_merged)\n",
    "\n",
    "    # transforms RNN output to character activations:\n",
    "    inner = Dense(num_classes, kernel_initializer='he_normal',name='dense2')(lstm2_merged) #(None, 32, 63)\n",
    "    y_pred = Activation('softmax', name='softmax')(inner)\n",
    "\n",
    "    labels = Input(name='the_labels', shape=[max_text_len], dtype='float32') # (None ,8)\n",
    "    input_length = Input(name='input_length', shape=[1], dtype='int64')     # (None, 1)\n",
    "    label_length = Input(name='label_length', shape=[1], dtype='int64')     # (None, 1)\n",
    "\n",
    "    # Keras doesn't currently support loss funcs with extra parameters\n",
    "    # so CTC loss is implemented in a lambda layer\n",
    "    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length]) #(None, 1)\n",
    "    \n",
    "    if training:\n",
    "        return Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)\n",
    "    else:\n",
    "        return Model(inputs=[inputs], outputs=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ultra\\Anaconda3\\envs\\ocr_env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Tensor(\"activation_4/Relu:0\", shape=(?, 23, 4, 256), dtype=float32)\n",
      "WARNING:tensorflow:From C:\\Users\\ultra\\Anaconda3\\envs\\ocr_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4249: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\ultra\\Anaconda3\\envs\\ocr_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4229: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          (None, 92, 32, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 92, 32, 64)   640         the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 92, 32, 64)   256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 92, 32, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max1 (MaxPooling2D)             (None, 46, 16, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 46, 16, 128)  73856       max1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 46, 16, 128)  512         conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 46, 16, 128)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max2 (MaxPooling2D)             (None, 23, 8, 128)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv6 (Conv2D)                  (None, 23, 8, 256)   131328      max2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 23, 8, 256)   1024        conv6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 23, 8, 256)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max4 (MaxPooling2D)             (None, 23, 4, 256)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "con7 (Conv2D)                   (None, 23, 4, 256)   262400      max4[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 23, 4, 256)   1024        con7[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 23, 4, 256)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 23, 1024)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 23, 128)      131200      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm1_b (LSTM)                  (None, 23, 256)      394240      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm1 (LSTM)                    (None, 23, 256)      394240      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 23, 256)      0           lstm1_b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 23, 256)      0           lstm1[0][0]                      \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 23, 256)      1024        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm2_b (LSTM)                  (None, 23, 256)      525312      batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lstm2 (LSTM)                    (None, 23, 256)      525312      batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 23, 256)      0           lstm2_b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 23, 512)      0           lstm2[0][0]                      \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 23, 12)       6156        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 23, 12)       0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           softmax[0][0]                    \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 2,448,524\n",
      "Trainable params: 2,446,604\n",
      "Non-trainable params: 1,920\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_Model(True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os, random\n",
    "import numpy as np\n",
    "\n",
    "# # Input data generator\n",
    "def labels_to_text(labels):     # letters의 index -> text (string)\n",
    "    return ''.join(list(map(lambda x: letters[int(x)], labels)))\n",
    "\n",
    "def text_to_labels(text):      # text를 letters 배열에서의 인덱스 값으로 변환\n",
    "    return list(map(lambda x: letters.index(x), text))\n",
    "\n",
    "\n",
    "class TextImageGenerator:\n",
    "    def __init__(self, img_dirpath, img_w, img_h,\n",
    "                 batch_size, downsample_factor, max_text_len=4):\n",
    "        self.img_h = img_h\n",
    "        self.img_w = img_w\n",
    "        self.batch_size = batch_size\n",
    "        self.max_text_len = max_text_len\n",
    "        self.downsample_factor = downsample_factor\n",
    "        self.img_dirpath = img_dirpath                  # image dir path\n",
    "        self.img_dir = os.listdir(self.img_dirpath)     # images list\n",
    "        self.n = len(self.img_dir)                      # number of images\n",
    "        self.indexes = list(range(self.n))\n",
    "        self.cur_index = 0\n",
    "        self.imgs = np.zeros((self.n, self.img_h, self.img_w))\n",
    "        self.texts = []\n",
    "\n",
    "    ## samples의 이미지 목록들을 opencv로 읽어 저장하기, texts에는 label 저장\n",
    "    def build_data(self):\n",
    "        print(self.n, \" Image Loading start...\")\n",
    "        for i, img_file in enumerate(self.img_dir):\n",
    "            img = cv2.imread(self.img_dirpath + img_file, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, (self.img_w, self.img_h))\n",
    "            img = img.astype(np.float32)\n",
    "            img = (img / 255.0) * 2.0 - 1.0\n",
    "            self.imgs[i, :, :] = img\n",
    "            img_file = img_file.split('_')[-1]\n",
    "            self.texts.append(img_file[0:-4])\n",
    "        print(len(self.texts) == self.n)\n",
    "        print(self.n, \" Image Loading finish...\")\n",
    "\n",
    "    def next_sample(self):      ## index max -> 0 으로 만들기\n",
    "        self.cur_index += 1\n",
    "        if self.cur_index >= self.n:\n",
    "            self.cur_index = 0\n",
    "            random.shuffle(self.indexes)\n",
    "        return self.imgs[self.indexes[self.cur_index]], self.texts[self.indexes[self.cur_index]]\n",
    "\n",
    "    def next_batch(self):       ## batch size만큼 가져오기\n",
    "        while True:\n",
    "            X_data = np.ones([self.batch_size, self.img_w, self.img_h, 1])     # (bs, 128, 64, 1)\n",
    "            Y_data = np.ones([self.batch_size, self.max_text_len])             # (bs, 9)\n",
    "            input_length = np.ones((self.batch_size, 1)) * (self.img_w // self.downsample_factor - 2)  # (bs, 1)\n",
    "            label_length = np.zeros((self.batch_size, 1))           # (bs, 1)\n",
    "\n",
    "            for i in range(self.batch_size):\n",
    "                img, text = self.next_sample()\n",
    "                img = img.T\n",
    "                img = np.expand_dims(img, -1)\n",
    "                X_data[i] = img\n",
    "                Y_data[i] = text_to_labels(text)\n",
    "                label_length[i] = len(text)\n",
    "#                 print(text)\n",
    "#                 print(f'Y_data[{i}] : {Y_data[i]}\\ninput_length : {input_length}\\n label_length[{i}]: {label_length[i]}\\n')\n",
    "            \n",
    "            # dict 형태로 복사\n",
    "            inputs = {\n",
    "                'the_input': X_data,  # (bs, 128, 64, 1) --> image\n",
    "                'the_labels': Y_data,  # (bs, 8) --> text를 label로 바꾼 값 (48 17 48 ...)같은\n",
    "                'input_length': input_length,  # (bs, 1) -> 모든 원소 value = 30 --> \n",
    "                'label_length': label_length  # (bs, 1) -> 모든 원소 value = 8 --> label의 길이\n",
    "            }\n",
    "            outputs = {'ctc': np.zeros([self.batch_size])}   # (bs, 1) -> 모든 원소 0\n",
    "            yield (inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"activation_8/Relu:0\", shape=(?, 23, 4, 256), dtype=float32)\n",
      "...New weight data...\n",
      "25920  Image Loading start...\n",
      "True\n",
      "25920  Image Loading finish...\n",
      "6480  Image Loading start...\n",
      "True\n",
      "6480  Image Loading finish...\n",
      "WARNING:tensorflow:From C:\\Users\\ultra\\Anaconda3\\envs\\ocr_env\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 606s 3s/step - loss: 10.4171 - val_loss: 8.5809\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 600s 3s/step - loss: 1.9687 - val_loss: 0.2094\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 595s 3s/step - loss: 0.2803 - val_loss: 0.0379\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 595s 3s/step - loss: 0.0943 - val_loss: 0.0120\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 596s 3s/step - loss: 0.0525 - val_loss: 0.5064\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 596s 3s/step - loss: 0.1266 - val_loss: 0.0052\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 597s 3s/step - loss: 0.0161 - val_loss: 0.0029\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 593s 3s/step - loss: 0.0243 - val_loss: 0.0044\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 581s 3s/step - loss: 0.0345 - val_loss: 0.0067\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 587s 3s/step - loss: 0.0271 - val_loss: 0.0047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24ccd11db00>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "\n",
    "# # Model description and training\n",
    "\n",
    "model = get_Model(training=True)\n",
    "\n",
    "try:\n",
    "    model.load_weights('LSTM+BN4--26--0.011.hdf5')\n",
    "    print(\"...Previous weight data...\")\n",
    "except:\n",
    "    print(\"...New weight data...\")\n",
    "    pass\n",
    "\n",
    "train_file_path = './data/train/'\n",
    "tiger_train = TextImageGenerator(train_file_path, img_w, img_h, batch_size, downsample_factor)\n",
    "tiger_train.build_data()\n",
    "\n",
    "valid_file_path = './data/test/'\n",
    "tiger_val = TextImageGenerator(valid_file_path, img_w, img_h, val_batch_size, downsample_factor)\n",
    "tiger_val.build_data()\n",
    "\n",
    "ada = Adadelta()\n",
    "# loss의 변화량이 0.001보다 적으면 학습의 개선이 없다 판단하고 4 epoch만큼 지속되면 종료\n",
    "early_stop = EarlyStopping(monitor='loss', min_delta=0.001, patience=4, mode='min', verbose=1)\n",
    "#checkpoint = ModelCheckpoint(filepath='LSTM+BN5--{epoch:02d}--{val_loss:.3f}.hdf5', monitor='loss', verbose=1, mode='min', period=1)\n",
    "# the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n",
    "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=ada)\n",
    "\n",
    "# generator로 생성된 배치로 학습할떄 fit_generator 사용\n",
    "# \n",
    "model.fit_generator(generator=tiger_train.next_batch(),\n",
    "                    steps_per_epoch=int(tiger_train.n / batch_size),\n",
    "                    epochs=10,\n",
    "                    #callbacks=[checkpoint],\n",
    "                    validation_data=tiger_val.next_batch(),\n",
    "                    validation_steps=int(tiger_val.n / val_batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./10_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import itertools, os, time\n",
    "import numpy as np\n",
    "import argparse\n",
    "from keras import backend as K\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "\n",
    "def decode_label(out):\n",
    "    # out : (1, 32, 42)\n",
    "#     print(f'decode_label.. out.shape : {out.shape}')\n",
    "    out_best = list(np.argmax(out[0, 2:], axis=1))  # get max index -> len = 32\n",
    "#     print(f'decode_label.. out_best : {out_best}')\n",
    "#    print(f'decode_label.. out_best.shape : {len(out_best)}')\n",
    "    out_best = [k for k, g in itertools.groupby(out_best)]  # remove overlap value\n",
    "#     print(f'decode_label..groupby.. out_best : {out_best}')\n",
    "    outstr = ''\n",
    "    for i in out_best:\n",
    "        if i < len(letters):\n",
    "            outstr += letters[i]\n",
    "    print(f'decode_label..groupby.. outstr : {outstr}')\n",
    "    return outstr\n",
    "\n",
    "\n",
    "# A99th3954같은 형태의 label이 예측되게 되는데 각 label의 위치값으로 번호판 예측\n",
    "def label_to_hangul(label):  # eng -> hangul\n",
    "    lb_text = ''\n",
    "    for lb in label:\n",
    "        if lb == 'd':\n",
    "            lb_text += '.'\n",
    "        else:\n",
    "            lb_text += lb\n",
    "    return lb_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_imgs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-ac9b13ea8435>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_imgs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtest_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIMREAD_GRAYSCALE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mimg_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_imgs' is not defined"
     ]
    }
   ],
   "source": [
    "test_img = test_imgs[0]\n",
    "img = cv2.imread(test_dir + test_img, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "img_pred = img.astype(np.float32)\n",
    "print(img_pred.shape)\n",
    "img_pred = cv2.resize(img_pred, (92, 32))\n",
    "print(img_pred.shape)\n",
    "img_pred = (img_pred / 69.0) * 2.0 - 1.0\n",
    "print(img_pred.shape)\n",
    "#print(img_pred.shape)\n",
    "img_pred = img_pred.T\n",
    "print(img_pred.shape)\n",
    "#print(img_pred.shape)\n",
    "img_pred = np.expand_dims(img_pred, axis=-1)\n",
    "print(img_pred.shape)\n",
    "img_pred = np.expand_dims(img_pred, axis=0)\n",
    "print(img_pred.shape)\n",
    "net_out_value = model.predict(img_pred)\n",
    "decode_label(net_out_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-3ce56e875a4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_img\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_img' is not defined"
     ]
    }
   ],
   "source": [
    "test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net_out_value' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-ef927541d02c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnet_out_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m22\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'net_out_value' is not defined"
     ]
    }
   ],
   "source": [
    "net_out_value[0,22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"activation_12/Relu:0\", shape=(?, 23, 4, 256), dtype=float32)\n",
      "...Previous weight data...\n",
      "decode_label..groupby.. outstr : d1d41d1d1d\n",
      "real label : 53d4.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : d4d41d41d4\n",
      "real label : 543d.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : d1d41d1d4d1\n",
      "real label : 54d2.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : d1d1d1d1\n",
      "real label : 54d7.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : d13d\n",
      "real label : 557d.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : 1d1d4d1d141d\n",
      "real label : 56d8.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : 3d1d1d1d14d31\n",
      "real label : 57d2.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : 3d141d1\n",
      "real label : 58d1.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : d14d41d41d4\n",
      "real label : 593d.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : 31d13d31d1d1\n",
      "real label : 596d.jpg\n",
      "***************\n"
     ]
    }
   ],
   "source": [
    "model = get_Model(training=False)\n",
    "\n",
    "try:\n",
    "    model.load_weights('10_model.h5')\n",
    "    print(\"...Previous weight data...\")\n",
    "except:\n",
    "    raise Exception(\"No weight file!\")\n",
    "test_dir = './data/test/'\n",
    "test_imgs = os.listdir(test_dir)\n",
    "total = 0\n",
    "acc = 0\n",
    "letter_total = 0\n",
    "letter_acc = 0\n",
    "start = time.time()\n",
    "for test_img in test_imgs[4501:4511]:\n",
    "    \n",
    "    img = cv2.imread(test_dir + test_img, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    img_pred = img.astype(np.float32)\n",
    "    img_pred = cv2.resize(img_pred, (92, 32))\n",
    "    img_pred = (img_pred / 69.0) * 2.0 - 1.0\n",
    "    #print(img_pred.shape)\n",
    "    img_pred = img_pred.T\n",
    "    #print(img_pred.shape)\n",
    "    img_pred = np.expand_dims(img_pred, axis=-1)\n",
    "    img_pred = np.expand_dims(img_pred, axis=0)\n",
    "    net_out_value = model.predict(img_pred)\n",
    "    decode_label(net_out_value)\n",
    "    real_label = test_img.split('_')[-1]\n",
    "    print(f'real label : {real_label}')\n",
    "    print('***************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decode_label..groupby.. outstr : d5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'d5'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_label(net_out_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decode_label.. out.shape : (1, 12, 14)\n",
      "decode_label.. out_best : [10, 10, 5, 5, 5, 5, 5, 10, 10, 10]\n",
      "decode_label.. out_best.shape : 10\n",
      "decode_label..groupby.. out_best : [10, 5, 10]\n",
      "decode_label..groupby.. outstr : d5d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'d5d'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_label(net_out_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 128, 64, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_Model(training=False)\n",
    "\n",
    "try:\n",
    "    model.load_weights('model.h5')\n",
    "    print(\"...Previous weight data...\")\n",
    "except:\n",
    "    raise Exception(\"No weight file!\")\n",
    "\n",
    "test_dir = './DB/test/'\n",
    "test_imgs = os.listdir(test_dir)\n",
    "total = 0\n",
    "acc = 0\n",
    "letter_total = 0\n",
    "letter_acc = 0\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "for test_img in test_imgs:\n",
    "    img = cv2.imread(test_dir + test_img, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    img_pred = img.astype(np.float32)\n",
    "    img_pred = cv2.resize(img_pred, (92, 32))\n",
    "    img_pred = (img_pred / 255.0) * 2.0 - 1.0\n",
    "    img_pred = img_pred.T\n",
    "    img_pred = np.expand_dims(img_pred, axis=-1)\n",
    "    img_pred = np.expand_dims(img_pred, axis=0)\n",
    "    # 1, 128, 64, 1\n",
    "    print(f'img_pred.shape : {img_pred.shape}')\n",
    "\n",
    "    net_out_value = model.predict(img_pred)\n",
    "\n",
    "    pred_texts = decode_label(net_out_value)\n",
    "    print(f'net_out_value.shape : {net_out_value.shape} pred_texts : {pred_texts}')\n",
    "    \n",
    "          \n",
    "    for i in range(min(len(pred_texts), len(test_img[0:-4]))):\n",
    "        if pred_texts[i] == test_img[i]:\n",
    "            letter_acc += 1\n",
    "    letter_total += max(len(pred_texts), len(test_img[0:-4]))\n",
    "\n",
    "    if pred_texts == test_img[0:-4]:\n",
    "        acc += 1\n",
    "    total += 1\n",
    "    print('Predicted: %s  /  True: %s' % (label_to_hangul(pred_texts), label_to_hangul(test_img[0:-4])))\n",
    "\n",
    "end = time.time()\n",
    "total_time = (end - start)\n",
    "print(\"Time : \",total_time / total)\n",
    "print(\"ACC : \", acc / total)\n",
    "print(\"letter ACC : \", letter_acc / letter_total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ocr_env] *",
   "language": "python",
   "name": "conda-env-ocr_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
