{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train / test 나누기\n",
    "- 한 번만 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './data/train'\n",
    "test_path = './data/test'\n",
    "\n",
    "train_set_len = int(len(os.listdir(train_path)) * 0.8) \n",
    "test_set_len = len(os.listdir(train_path)) - train_set_len\n",
    "\n",
    "file_list = []\n",
    "for file in os.listdir(train_path):\n",
    "    file_path = f'{train_path}/{file}'\n",
    "    file_list.append(file_path)\n",
    "    \n",
    "for file in random.sample(file_list, test_set_len):\n",
    "    move_dir = f'{test_path}{file[0][12:]}'\n",
    "    shutil.move(file, move_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9600"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_VECTOR = \"0123456789dfghijklmno\"\n",
    "# one-hot으로 쓰기위한 letters \n",
    "\n",
    "letters = [letter for letter in CHAR_VECTOR]\n",
    "\n",
    "num_classes = len(letters) + 1\n",
    "\n",
    "img_w, img_h = 160, 28\n",
    "\n",
    "# Network parameters\n",
    "# batch_size = 128\n",
    "# val_batch_size = 16\n",
    "batch_size = 64\n",
    "val_batch_size = 16\n",
    "\n",
    "downsample_factor = 2\n",
    "max_text_len = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.layers import Reshape, Lambda, BatchNormalization\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.recurrent import LSTM\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "# # Loss and train functions, network architecture\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    # the 2 is critical here since the first couple outputs of the RNN\n",
    "    # tend to be garbage:\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "\n",
    "def get_Model(training):\n",
    "    input_shape = (img_w, img_h, 1)     # (160, 28, 1)\n",
    "\n",
    "    # Make Networkw\n",
    "    inputs = Input(name='the_input', shape=input_shape, dtype='float32')  # (None, 160, 28, 1)\n",
    "\n",
    "    inner = Conv2D(64, (2, 2), padding='same', name='conv1', kernel_initializer='he_normal')(inputs)  # (None, 48, 56, 32)\n",
    "    inner = BatchNormalization()(inner)\n",
    "    inner = Activation('relu')(inner)\n",
    "    inner = MaxPooling2D(pool_size=(2, 2), name='max1')(inner)  # (None, 80, 14, 64)\n",
    "    print(inner)\n",
    "    inner = Conv2D(256, (2, 2), padding='same', name='conv2')(inner)  # \n",
    "    inner = BatchNormalization()(inner)\n",
    "    inner = Activation('relu')(inner)\n",
    "    inner = MaxPooling2D(pool_size=(1, 2), name='max2')(inner)  # (None, 80, 7, 256)\n",
    "    print(inner)\n",
    "    inner = Conv2D(256, (2, 2), padding='same', kernel_initializer='he_normal', name='conv3')(inner)  # (None, 32, 4, 512)\n",
    "    inner = BatchNormalization()(inner)\n",
    "    inner = Activation('relu')(inner) # (?, 80, 7, 256)\n",
    "    print(inner)\n",
    "    # CNN to RNN\n",
    "    inner = Reshape(target_shape=((80, 7*256)), name='reshape')(inner)  # (None, 32, 2048)\n",
    "    inner = Dense(512, activation='relu', kernel_initializer='he_normal', name='dense1')(inner)  # (None, 80, 1792)\n",
    "\n",
    "    # RNN layer\n",
    "    lstm_1 = LSTM(256, return_sequences=True, kernel_initializer='he_normal', name='lstm1')(inner)  # (None, 32, 512)\n",
    "    lstm_1b = LSTM(256, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='lstm1_b')(inner)\n",
    "    reversed_lstm_1b = Lambda(lambda inputTensor: K.reverse(inputTensor, axes=1)) (lstm_1b)\n",
    "\n",
    "    lstm1_merged = add([lstm_1, reversed_lstm_1b])  # (None, 32, 512)\n",
    "    lstm1_merged = BatchNormalization()(lstm1_merged)\n",
    "    \n",
    "    lstm_2 = LSTM(256, return_sequences=True, kernel_initializer='he_normal', name='lstm2')(lstm1_merged)\n",
    "    lstm_2b = LSTM(256, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='lstm2_b')(lstm1_merged)\n",
    "    reversed_lstm_2b= Lambda(lambda inputTensor: K.reverse(inputTensor, axes=1)) (lstm_2b)\n",
    "\n",
    "    lstm2_merged = concatenate([lstm_2, reversed_lstm_2b])  # (None, 32, 1024)\n",
    "    lstm_merged = BatchNormalization()(lstm2_merged)\n",
    "\n",
    "    # transforms RNN output to character activations:\n",
    "    inner = Dense(num_classes, kernel_initializer='he_normal',name='dense2')(lstm2_merged) #(None, 32, 63)\n",
    "    y_pred = Activation('softmax', name='softmax')(inner)\n",
    "\n",
    "    labels = Input(name='the_labels', shape=[max_text_len], dtype='float32') # (None ,8)\n",
    "    input_length = Input(name='input_length', shape=[1], dtype='int64')     # (None, 1)\n",
    "    label_length = Input(name='label_length', shape=[1], dtype='int64')     # (None, 1)\n",
    "\n",
    "    # Keras doesn't currently support loss funcs with extra parameters\n",
    "    # so CTC loss is implemented in a lambda layer\n",
    "    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length]) #(None, 1)\n",
    "    \n",
    "    if training:\n",
    "        return Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)\n",
    "    else:\n",
    "        return Model(inputs=[inputs], outputs=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ultra\\Anaconda3\\envs\\ocr_env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Tensor(\"max1/MaxPool:0\", shape=(?, 80, 14, 64), dtype=float32)\n",
      "Tensor(\"max2/MaxPool:0\", shape=(?, 80, 7, 256), dtype=float32)\n",
      "Tensor(\"activation_3/Relu:0\", shape=(?, 80, 7, 256), dtype=float32)\n",
      "WARNING:tensorflow:From C:\\Users\\ultra\\Anaconda3\\envs\\ocr_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4249: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\ultra\\Anaconda3\\envs\\ocr_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4229: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          (None, 160, 28, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 160, 28, 64)  320         the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 160, 28, 64)  256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 160, 28, 64)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max1 (MaxPooling2D)             (None, 80, 14, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 80, 14, 256)  65792       max1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 80, 14, 256)  1024        conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 80, 14, 256)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max2 (MaxPooling2D)             (None, 80, 7, 256)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                  (None, 80, 7, 256)   262400      max2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 80, 7, 256)   1024        conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 80, 7, 256)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 80, 1792)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 80, 512)      918016      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm1_b (LSTM)                  (None, 80, 256)      787456      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm1 (LSTM)                    (None, 80, 256)      787456      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 80, 256)      0           lstm1_b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 80, 256)      0           lstm1[0][0]                      \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 80, 256)      1024        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm2_b (LSTM)                  (None, 80, 256)      525312      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lstm2 (LSTM)                    (None, 80, 256)      525312      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 80, 256)      0           lstm2_b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 80, 512)      0           lstm2[0][0]                      \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 80, 22)       11286       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 80, 22)       0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           softmax[0][0]                    \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 3,886,678\n",
      "Trainable params: 3,885,014\n",
      "Non-trainable params: 1,664\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_Model(True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os, random\n",
    "import numpy as np\n",
    "\n",
    "# # Input data generator\n",
    "def labels_to_text(labels):     # letters의 index -> text (string)\n",
    "    return ''.join(list(map(lambda x: letters[int(x)], labels)))\n",
    "\n",
    "def text_to_labels(text):      # text를 letters 배열에서의 인덱스 값으로 변환\n",
    "    return list(map(lambda x: letters.index(x), text))\n",
    "\n",
    "\n",
    "class TextImageGenerator:\n",
    "    def __init__(self, img_dirpath, img_w, img_h,\n",
    "                 batch_size, downsample_factor, max_text_len=4):\n",
    "        self.img_h = img_h\n",
    "        self.img_w = img_w\n",
    "        self.batch_size = batch_size\n",
    "        self.max_text_len = max_text_len\n",
    "        self.downsample_factor = downsample_factor\n",
    "        self.img_dirpath = img_dirpath                  # image dir path\n",
    "        self.img_dir = os.listdir(self.img_dirpath)     # images list\n",
    "        self.n = len(self.img_dir)                      # number of images\n",
    "        self.indexes = list(range(self.n))\n",
    "        self.cur_index = 0\n",
    "        self.imgs = np.zeros((self.n, self.img_h, self.img_w))\n",
    "        self.texts = []\n",
    "\n",
    "    ## samples의 이미지 목록들을 opencv로 읽어 저장하기, texts에는 label 저장\n",
    "    def build_data(self):\n",
    "        print(self.n, \" Image Loading start...\")\n",
    "        for i, img_file in enumerate(self.img_dir):\n",
    "            img = cv2.imread(self.img_dirpath + img_file, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, (self.img_w, self.img_h))\n",
    "            img = img.astype(np.float32)\n",
    "            img = (img / 255.0) * 2.0 - 1.0\n",
    "            self.imgs[i, :, :] = img\n",
    "            img_file = img_file.split('_')[-1]\n",
    "            self.texts.append(img_file[0:-4])\n",
    "        print(len(self.texts) == self.n)\n",
    "        print(self.n, \" Image Loading finish...\")\n",
    "\n",
    "    def next_sample(self):      ## index max -> 0 으로 만들기\n",
    "        self.cur_index += 1\n",
    "        if self.cur_index >= self.n:\n",
    "            self.cur_index = 0\n",
    "            random.shuffle(self.indexes)\n",
    "        return self.imgs[self.indexes[self.cur_index]], self.texts[self.indexes[self.cur_index]]\n",
    "\n",
    "    def next_batch(self):       ## batch size만큼 가져오기\n",
    "        while True:\n",
    "            X_data = np.ones([self.batch_size, self.img_w, self.img_h, 1])     # (bs, 128, 64, 1)\n",
    "            Y_data = np.ones([self.batch_size, self.max_text_len])             # (bs, 9)\n",
    "            input_length = np.ones((self.batch_size, 1)) * (self.img_w // self.downsample_factor - 2)  # (bs, 1)\n",
    "            label_length = np.zeros((self.batch_size, 1))           # (bs, 1)\n",
    "\n",
    "            for i in range(self.batch_size):\n",
    "                img, text = self.next_sample()\n",
    "                img = img.T\n",
    "                img = np.expand_dims(img, -1)\n",
    "                X_data[i] = img\n",
    "                Y_data[i] = text_to_labels(text)\n",
    "                label_length[i] = len(text)\n",
    "#                 print(text)\n",
    "#                 print(f'Y_data[{i}] : {Y_data[i]}\\ninput_length : {input_length}\\n label_length[{i}]: {label_length[i]}\\n')\n",
    "            \n",
    "            # dict 형태로 복사\n",
    "            inputs = {\n",
    "                'the_input': X_data,  # (bs, 128, 64, 1) --> image\n",
    "                'the_labels': Y_data,  # (bs, 8) --> text를 label로 바꾼 값 (48 17 48 ...)같은\n",
    "                'input_length': input_length,  # (bs, 1) -> 모든 원소 value = 30 --> \n",
    "                'label_length': label_length  # (bs, 1) -> 모든 원소 value = 8 --> label의 길이\n",
    "            }\n",
    "            outputs = {'ctc': np.zeros([self.batch_size])}   # (bs, 1) -> 모든 원소 0\n",
    "            yield (inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"max1_1/MaxPool:0\", shape=(?, 80, 14, 64), dtype=float32)\n",
      "Tensor(\"max2_1/MaxPool:0\", shape=(?, 80, 7, 256), dtype=float32)\n",
      "Tensor(\"activation_6/Relu:0\", shape=(?, 80, 7, 256), dtype=float32)\n",
      "38400  Image Loading start...\n",
      "True\n",
      "38400  Image Loading finish...\n",
      "9600  Image Loading start...\n",
      "True\n",
      "9600  Image Loading finish...\n",
      "WARNING:tensorflow:From C:\\Users\\ultra\\Anaconda3\\envs\\ocr_env\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 956s 2s/step - loss: 15.0069 - val_loss: 18.5316\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 911s 2s/step - loss: 13.9556 - val_loss: 9.7696\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 903s 2s/step - loss: 6.2440 - val_loss: 9.7113\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 913s 2s/step - loss: 3.8480 - val_loss: 0.0767\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 908s 2s/step - loss: 2.0583 - val_loss: 0.0194\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 898s 1s/step - loss: 0.0354 - val_loss: 0.0105\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 906s 2s/step - loss: 2.0582 - val_loss: 0.0373\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 902s 2s/step - loss: 0.1656 - val_loss: 0.0016\n",
      "Epoch 9/20\n",
      "600/600 [==============================] - 906s 2s/step - loss: 0.2026 - val_loss: 0.0037\n",
      "Epoch 10/20\n",
      "600/600 [==============================] - 908s 2s/step - loss: 0.0011 - val_loss: 4.3210e-04\n",
      "Epoch 11/20\n",
      "600/600 [==============================] - 900s 2s/step - loss: 0.0014 - val_loss: 3.4096e-04\n",
      "Epoch 12/20\n",
      "600/600 [==============================] - 900s 2s/step - loss: 3.0798e-04 - val_loss: 2.9112e-04\n",
      "Epoch 13/20\n",
      "600/600 [==============================] - 901s 2s/step - loss: 2.6819e-04 - val_loss: 2.5803e-04\n",
      "Epoch 14/20\n",
      " 27/600 [>.............................] - ETA: 11:37 - loss: 2.5941e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-4146d76355eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m                     \u001b[1;31m#callbacks=[checkpoint],\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtiger_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                     validation_steps=int(tiger_val.n / val_batch_size))\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\ocr_env\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ocr_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ocr_env\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ocr_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ocr_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ocr_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ocr_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "\n",
    "# # Model description and training\n",
    "\n",
    "model = get_Model(training=True)\n",
    "\n",
    "train_file_path = './data/train/'\n",
    "tiger_train = TextImageGenerator(train_file_path, img_w, img_h, batch_size, downsample_factor)\n",
    "tiger_train.build_data()\n",
    "\n",
    "valid_file_path = './data/test/'\n",
    "tiger_val = TextImageGenerator(valid_file_path, img_w, img_h, val_batch_size, downsample_factor)\n",
    "tiger_val.build_data()\n",
    "\n",
    "ada = Adadelta()\n",
    "# loss의 변화량이 0.001보다 적으면 학습의 개선이 없다 판단하고 4 epoch만큼 지속되면 종료\n",
    "early_stop = EarlyStopping(monitor='loss', min_delta=0.001, patience=4, mode='min', verbose=1)\n",
    "#checkpoint = ModelCheckpoint(filepath='LSTM+BN5--{epoch:02d}--{val_loss:.3f}.hdf5', monitor='loss', verbose=1, mode='min', period=1)\n",
    "# the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n",
    "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=ada)\n",
    "\n",
    "# generator로 생성된 배치로 학습할떄 fit_generator 사용\n",
    "# \n",
    "model.fit_generator(generator=tiger_train.next_batch(),\n",
    "                    steps_per_epoch=int(tiger_train.n / batch_size),\n",
    "                    epochs=20,\n",
    "                    #callbacks=[checkpoint],\n",
    "                    validation_data=tiger_val.next_batch(),\n",
    "                    validation_steps=int(tiger_val.n / val_batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./13_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import itertools, os, time\n",
    "import numpy as np\n",
    "import argparse\n",
    "from keras import backend as K\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "\n",
    "def decode_label(out):\n",
    "    # out : (1, 32, 42)\n",
    "#     print(f'decode_label.. out.shape : {out.shape}')\n",
    "    out_best = list(np.argmax(out[0, 2:], axis=1))  # get max index -> len = 32\n",
    "#     print(f'decode_label.. out_best : {out_best}')\n",
    "#    print(f'decode_label.. out_best.shape : {len(out_best)}')\n",
    "    out_best = [k for k, g in itertools.groupby(out_best)]  # remove overlap value\n",
    "#     print(f'decode_label..groupby.. out_best : {out_best}')\n",
    "    outstr = ''\n",
    "    for i in out_best:\n",
    "        if i < len(letters):\n",
    "            outstr += letters[i]\n",
    "    print(f'decode_label..groupby.. outstr : {outstr}')\n",
    "    return outstr\n",
    "\n",
    "\n",
    "# A99th3954같은 형태의 label이 예측되게 되는데 각 label의 위치값으로 번호판 예측\n",
    "def label_to_hangul(label):  # eng -> hangul\n",
    "    lb_text = ''\n",
    "    for lb in label:\n",
    "        if lb == 'd':\n",
    "            lb_text += '.'\n",
    "        else:\n",
    "            lb_text += lb\n",
    "    return lb_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"max1_2/MaxPool:0\", shape=(?, 80, 14, 64), dtype=float32)\n",
      "Tensor(\"max2_2/MaxPool:0\", shape=(?, 80, 7, 256), dtype=float32)\n",
      "Tensor(\"activation_9/Relu:0\", shape=(?, 80, 7, 256), dtype=float32)\n",
      "...Previous weight data...\n",
      "decode_label..groupby.. outstr : 4\n",
      "real label : 64.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : 694\n",
      "real label : 694.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : 8d2\n",
      "real label : 8d2.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : 3308\n",
      "real label : 3308.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : 3886\n",
      "real label : 3886.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : 5d3\n",
      "real label : 5d3.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : 7414\n",
      "real label : 7414.jpg\n",
      "***************\n"
     ]
    }
   ],
   "source": [
    "model = get_Model(training=False)\n",
    "\n",
    "try:\n",
    "    model.load_weights('13_model.h5')\n",
    "    print(\"...Previous weight data...\")\n",
    "except:\n",
    "    raise Exception(\"No weight file!\")\n",
    "test_dir = './data/test2/'\n",
    "test_imgs = os.listdir(test_dir)\n",
    "total = 0\n",
    "acc = 0\n",
    "letter_total = 0\n",
    "letter_acc = 0\n",
    "start = time.time()\n",
    "for test_img in test_imgs:\n",
    "    \n",
    "    img = cv2.imread(test_dir + test_img, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    img_pred = img.astype(np.float32)\n",
    "    img_pred = cv2.resize(img_pred, (160, 28))\n",
    "    img_pred = (img_pred / 255.0) * 2.0 - 1.0\n",
    "    #print(img_pred.shape)\n",
    "    img_pred = img_pred.T\n",
    "    #print(img_pred.shape)\n",
    "    img_pred = np.expand_dims(img_pred, axis=-1)\n",
    "    img_pred = np.expand_dims(img_pred, axis=0)\n",
    "    net_out_value = model.predict(img_pred)\n",
    "    decode_label(net_out_value)\n",
    "    real_label = test_img.split('_')[-1]\n",
    "    print(f'real label : {real_label}')\n",
    "    print('***************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"max1_5/MaxPool:0\", shape=(?, 80, 14, 64), dtype=float32)\n",
      "Tensor(\"max2_5/MaxPool:0\", shape=(?, 80, 7, 256), dtype=float32)\n",
      "Tensor(\"activation_18/Relu:0\", shape=(?, 80, 7, 256), dtype=float32)\n",
      "...Previous weight data...\n",
      "decode_label..groupby.. outstr : ommf\n",
      "real label : ommf.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : ommh\n",
      "real label : ommh.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : ommo\n",
      "real label : ommo.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : omoh\n",
      "real label : omoh.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : ongn\n",
      "real label : ongn.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : ongo\n",
      "real label : ongo.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : onkg\n",
      "real label : onkg.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : onmm\n",
      "real label : onmm.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : onmn\n",
      "real label : onmn.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : onok\n",
      "real label : onok.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : oofn\n",
      "real label : oofn.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : oojl\n",
      "real label : oojl.jpg\n",
      "***************\n"
     ]
    }
   ],
   "source": [
    "model = get_Model(training=False)\n",
    "\n",
    "try:\n",
    "    model.load_weights('13_model.h5')\n",
    "    print(\"...Previous weight data...\")\n",
    "except:\n",
    "    raise Exception(\"No weight file!\")\n",
    "test_dir = './data/test/'\n",
    "test_imgs = os.listdir(test_dir)\n",
    "total = 0\n",
    "acc = 0\n",
    "letter_total = 0\n",
    "letter_acc = 0\n",
    "start = time.time()\n",
    "for test_img in test_imgs[-12:]:\n",
    "    \n",
    "    img = cv2.imread(test_dir + test_img, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    img_pred = img.astype(np.float32)\n",
    "    img_pred = cv2.resize(img_pred, (160, 28))\n",
    "    img_pred = (img_pred / 255.0) * 2.0 - 1.0\n",
    "    #print(img_pred.shape)\n",
    "    img_pred = img_pred.T\n",
    "    #print(img_pred.shape)\n",
    "    img_pred = np.expand_dims(img_pred, axis=-1)\n",
    "    img_pred = np.expand_dims(img_pred, axis=0)\n",
    "    net_out_value = model.predict(img_pred)\n",
    "    decode_label(net_out_value)\n",
    "    real_label = test_img.split('_')[-1]\n",
    "    print(f'real label : {real_label}')\n",
    "    print('***************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"max1_4/MaxPool:0\", shape=(?, 80, 14, 64), dtype=float32)\n",
      "Tensor(\"max2_4/MaxPool:0\", shape=(?, 80, 7, 256), dtype=float32)\n",
      "Tensor(\"activation_15/Relu:0\", shape=(?, 80, 7, 256), dtype=float32)\n",
      "...Previous weight data...\n",
      "decode_label..groupby.. outstr : 694\n",
      "real label : 694.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : 8d2\n",
      "real label : 8d2.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : 3308\n",
      "real label : 3308.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : 3886\n",
      "real label : 3886.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : 5d3\n",
      "real label : 5d3.jpg\n",
      "***************\n",
      "decode_label..groupby.. outstr : 7414\n",
      "real label : 7414.jpg\n",
      "***************\n"
     ]
    }
   ],
   "source": [
    "model = get_Model(training=False)\n",
    "\n",
    "try:\n",
    "    model.load_weights('13_model.h5')\n",
    "    print(\"...Previous weight data...\")\n",
    "except:\n",
    "    raise Exception(\"No weight file!\")\n",
    "test_dir = './data/test2/'\n",
    "test_imgs = os.listdir(test_dir)\n",
    "total = 0\n",
    "acc = 0\n",
    "letter_total = 0\n",
    "letter_acc = 0\n",
    "start = time.time()\n",
    "for test_img in test_imgs[1:12]:\n",
    "    \n",
    "    img = cv2.imread(test_dir + test_img, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    img_pred = img.astype(np.float32)\n",
    "    img_pred = cv2.resize(img_pred, (160, 28))\n",
    "    img_pred = (img_pred / 255.0) * 2.0 - 1.0\n",
    "    #print(img_pred.shape)\n",
    "    img_pred = img_pred.T\n",
    "    #print(img_pred.shape)\n",
    "    img_pred = np.expand_dims(img_pred, axis=-1)\n",
    "    img_pred = np.expand_dims(img_pred, axis=0)\n",
    "    net_out_value = model.predict(img_pred)\n",
    "    decode_label(net_out_value)\n",
    "    real_label = test_img.split('_')[-1]\n",
    "    print(f'real label : {real_label}')\n",
    "    print('***************')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ocr_env] *",
   "language": "python",
   "name": "conda-env-ocr_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
